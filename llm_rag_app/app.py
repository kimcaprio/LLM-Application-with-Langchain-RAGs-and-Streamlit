"""
Streamlit ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜

ì´ ëª¨ë“ˆì€ Langchain ê¸°ë°˜ RAG ì‹œìŠ¤í…œê³¼ LLM ëª¨ë¸ì„ í†µí•©í•œ Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
openwebui ë””ìì¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤íƒ€ì¼ë§ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import os
import streamlit as st
from dotenv import load_dotenv
import time
from rag_system import RAGSystem
from llm_api import DualModelChain
from styles import openwebui_css

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LLM RAG ì• í”Œë¦¬ì¼€ì´ì…˜",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# openwebui ìŠ¤íƒ€ì¼ ì ìš©
st.markdown(f'<style>{openwebui_css}</style>', unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "rag_system" not in st.session_state:
    st.session_state.rag_system = RAGSystem()

if "llm_chain" not in st.session_state:
    st.session_state.llm_chain = DualModelChain()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "vector_store_path" not in st.session_state:
    st.session_state.vector_store_path = "./vector_store"

# ì‚¬ì´ë“œë°” - ë¬¸ì„œ ì—…ë¡œë“œ ë° ê´€ë¦¬
with st.sidebar:
    st.title("ğŸ“š ë¬¸ì„œ ê´€ë¦¬")
    
    # ë¬¸ì„œ ì—…ë¡œë“œ
    st.header("ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader("í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ", type=["txt"], accept_multiple_files=True)
    
    if uploaded_files:
        if st.button("ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”©"):
            with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
                # ì„ì‹œ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì €ì¥
                temp_dir = "./temp_docs"
                os.makedirs(temp_dir, exist_ok=True)
                
                for file in uploaded_files:
                    file_path = os.path.join(temp_dir, file.name)
                    with open(file_path, "wb") as f:
                        f.write(file.getbuffer())
                
                # ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”©
                st.session_state.rag_system.add_documents_from_directory(
                    temp_dir, 
                    save_path=st.session_state.vector_store_path
                )
                
                st.success(f"{len(uploaded_files)}ê°œì˜ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€
    st.header("ìƒ˜í”Œ ë°ì´í„°")
    if st.button("ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€"):
        with st.spinner("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì¶”ê°€ ì¤‘ì…ë‹ˆë‹¤..."):
            sample_texts = [
                "ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµ, ì¶”ë¡ , ì§€ê°, ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ ë“±ì„ ì»´í“¨í„° í”„ë¡œê·¸ë¨ìœ¼ë¡œ êµ¬í˜„í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.",
                "ë¨¸ì‹ ëŸ¬ë‹ì€ ì»´í“¨í„°ê°€ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆê²Œ í•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
                "ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ì¢…ë¥˜ë¡œ, ë³µì¡í•œ íŒ¨í„´ì„ ì¸ì‹í•˜ëŠ” ë° ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.",
                "ìì—°ì–´ ì²˜ë¦¬(NLP)ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ë¡œ, ë²ˆì—­, ê°ì • ë¶„ì„, í…ìŠ¤íŠ¸ ìš”ì•½ ë“±ì— í™œìš©ë©ë‹ˆë‹¤.",
                "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ëª¨ë¸ë¡œ, ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë” ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.",
                "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì„ë² ë”©ëœ ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë² ì´ìŠ¤ë¡œ, ìœ ì‚¬ë„ ê²€ìƒ‰ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "FAISSëŠ” Facebook AIì—ì„œ ê°œë°œí•œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ëŒ€ê·œëª¨ ë²¡í„° ë°ì´í„°ì…‹ì—ì„œ íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.",
                "Langchainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¡œ, ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ]
            
            sample_metadatas = [
                {"source": "AI ê°œìš”", "author": "ê¹€ì¸ê³µ"},
                {"source": "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ", "author": "ì´í•™ìŠµ"},
                {"source": "ë”¥ëŸ¬ë‹ ì†Œê°œ", "author": "ë°•ì‹ ê²½ë§"},
                {"source": "NLP ê¸°ìˆ ", "author": "ìµœì–¸ì–´"},
                {"source": "RAG ì‹œìŠ¤í…œ", "author": "ì •ê²€ìƒ‰"},
                {"source": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤", "author": "ê°•ë²¡í„°"},
                {"source": "FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬", "author": "ì„ìœ ì‚¬ë„"},
                {"source": "Langchain í”„ë ˆì„ì›Œí¬", "author": "ì˜¤ì²´ì¸"}
            ]
            
            st.session_state.rag_system.add_texts(
                sample_texts, 
                sample_metadatas, 
                save_path=st.session_state.vector_store_path
            )
            
            st.success("ìƒ˜í”Œ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì‹œìŠ¤í…œ ì •ë³´
    st.header("ì‹œìŠ¤í…œ ì •ë³´")
    st.info("""
    **ëª¨ë¸ ì •ë³´:**
    - ì¶”ë¡  ëª¨ë¸: deepseek-r1:7b
    - ì¶œë ¥ ëª¨ë¸: exaone3.5
    - ì„ë² ë”© ëª¨ë¸: all-MiniLM-L6-v2
    
    **ë²¡í„° ì €ì¥ì†Œ:**
    - FAISS (Facebook AI Similarity Search)
    """)

# ë©”ì¸ í™”ë©´ - ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ¤– LLM RAG ì• í”Œë¦¬ì¼€ì´ì…˜")
st.markdown("""
ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ Langchain ê¸°ë°˜ RAG(Retrieval-Augmented Generation)ë¥¼ í™œìš©í•œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
- **deepseek-r1:7b** ëª¨ë¸ì€ ì¶”ë¡  ê³¼ì •ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
- **exaone3.5** ëª¨ë¸ì€ ìµœì¢… ì¶œë ¥ ìƒì„±ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
- **FAISS**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì„ë² ë”©ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.
""")

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.write(message["content"])
        
        # ì¶”ë¡  ê³¼ì • í‘œì‹œ (ì‚¬ìš©ìê°€ ìš”ì²­í•œ ê²½ìš°)
        if message["role"] == "assistant" and "reasoning" in message:
            with st.expander("ì¶”ë¡  ê³¼ì • ë³´ê¸°"):
                st.write(message["reasoning"])

# ì‚¬ìš©ì ì…ë ¥
user_query = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_query:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.chat_message("user").write(user_query)
    st.session_state.chat_history.append({"role": "user", "content": user_query})
    
    # ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ì‘ë‹µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            try:
                retrieved_docs = st.session_state.rag_system.query(user_query, k=3)
                
                # ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€
                if not retrieved_docs:
                    st.warning("ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
                    sample_texts = [
                        "ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµ, ì¶”ë¡ , ì§€ê°, ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ ë“±ì„ ì»´í“¨í„° í”„ë¡œê·¸ë¨ìœ¼ë¡œ êµ¬í˜„í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.",
                        "ë¨¸ì‹ ëŸ¬ë‹ì€ ì»´í“¨í„°ê°€ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆê²Œ í•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤."
                    ]
                    st.session_state.rag_system.add_texts(sample_texts, save_path=st.session_state.vector_store_path)
                    retrieved_docs = st.session_state.rag_system.query(user_query, k=2)
                
                # LLM ì²´ì¸ ì‹¤í–‰
                result = st.session_state.llm_chain.run(user_query, retrieved_docs)
                
                # ì‘ë‹µ í‘œì‹œ
                st.write(result["response"])
                
                # ì¶”ë¡  ê³¼ì • í‘œì‹œ
                with st.expander("ì¶”ë¡  ê³¼ì • ë³´ê¸°"):
                    st.write(result["reasoning"])
                
                # ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ
                with st.expander("ì°¸ì¡° ë¬¸ì„œ ë³´ê¸°"):
                    for i, doc in enumerate(retrieved_docs):
                        st.markdown(f"**ë¬¸ì„œ {i+1}:** {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                        st.markdown(f"**ì‘ì„±ì:** {doc.metadata.get('author', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                        st.markdown(f"**ë‚´ìš©:** {doc.page_content}")
                        st.markdown("---")
                
                # ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
                st.session_state.chat_history.append({
                    "role": "assistant", 
                    "content": result["response"],
                    "reasoning": result["reasoning"]
                })
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                st.info("ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ê±°ë‚˜ ì‚¬ì´ë“œë°”ì—ì„œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")

# í‘¸í„°
st.markdown("---")
st.markdown("Â© 2025 LLM RAG ì• í”Œë¦¬ì¼€ì´ì…˜ | Powered by Langchain, FAISS, Streamlit")
